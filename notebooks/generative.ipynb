{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3489e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade torch accelerate kernels\n",
    "# !pip install -q git+https://github.com/huggingface/transformers triton==3.4 git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "# !pip uninstall -q torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f4b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_train_set = False\n",
    "gpt_dev_set = False\n",
    "mistral01 = False\n",
    "mistral03 = True\n",
    "llama31 = False\n",
    "qwen2 = False\n",
    "\n",
    "\n",
    "train_minerva7b = False\n",
    "eval_minerva7b = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f602f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import paths\n",
    "from huggingface_hub import login\n",
    "#Token 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\n",
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "082d97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Mxfp4Config\n",
    "if gpt_train_set or gpt_dev_set:\n",
    "\n",
    "    model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    quantization_config=Mxfp4Config.from_dict(config.quantization_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb7164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74764\n",
      "74764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6630260199164792,\n",
       " 'recall': 0.8434818144666939,\n",
       " 'f1': 0.7424460431654676,\n",
       " 'accuracy': 0.9808463966614949}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "import gptoss_sent_split\n",
    "importlib.reload(gptoss_sent_split)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "if gpt_train_set:\n",
    "\n",
    "    cfg = BOSConfig(max_new_tokens=256)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_train_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'gptpredtrain.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "import pickle\n",
    "with open(paths.results/'gptpredtrain.pkl', 'rb') as f:\n",
    "    y_pred = pickle.load(f)\n",
    "\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_train_tokens.csv\")\n",
    "gold = [y for (_, y) in pairs]\n",
    "print(len(gold))\n",
    "print(len(y_pred))\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    ")\n",
    "acc = accuracy_score(gold, y_pred)\n",
    "{\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0974929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343\n",
      "9343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6625,\n",
       " 'recall': 0.8179012345679012,\n",
       " 'f1': 0.7320441988950276,\n",
       " 'accuracy': 0.979235791501659}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "import gptoss_sent_split\n",
    "importlib.reload(gptoss_sent_split)\n",
    "from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "if gpt_dev_set:\n",
    "\n",
    "    cfg = BOSConfig(max_new_tokens=256)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'gptpredval.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "with open(paths.results/'gptpredval.pkl', 'rb') as f:\n",
    "    y_pred = pickle.load(f)\n",
    "\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "gold = [y for (_, y) in pairs]\n",
    "print(len(gold))\n",
    "print(len(y_pred))\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    ")\n",
    "acc = accuracy_score(gold, y_pred)\n",
    "{\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d2af0",
   "metadata": {},
   "source": [
    "# Similar models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mistral01:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_id,\n",
    "    #     torch_dtype=\"auto\",\n",
    "    #     device_map=\"cuda\",\n",
    "    # )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=512, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    # y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    # tokens = [t for (t                      , _) in pairs]\n",
    "    # gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # # Align lengths, just in case\n",
    "    # n = min(len(tokens), len(y_pred))\n",
    "    # tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    # sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    # import pickle\n",
    "    # with open(paths.results/'Mistral-7B-Instruct-v0.1-dev.pkl', 'wb') as f:\n",
    "    #     pickle.dump((y_pred, skipped_jobs), f)\n",
    "    # import pickle\n",
    "    with open(paths.results/'Mistral-7B-Instruct-v0.1-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    # Create set of all token indices in skipped jobs\n",
    "    skipped_token_indices = set()\n",
    "    for job_idx in skipped_jobs:\n",
    "        job = jobs[job_idx]\n",
    "        start_token = job[\"start\"]\n",
    "        end_token = start_token + len(job[\"tokens\"])  # All tokens in this job\n",
    "        skipped_token_indices.update(range(start_token, end_token))\n",
    "\n",
    "    # Create new gold and pred lists excluding tokens from skipped jobs\n",
    "    new_gold = [label for idx, label in enumerate(gold) if idx not in skipped_token_indices]\n",
    "    new_y_pred = [pred for idx, pred in enumerate(y_pred) if idx not in skipped_token_indices]\n",
    "\n",
    "    # Second evaluation: only non-skipped tokens\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        new_gold, new_y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(new_gold, new_y_pred)\n",
    "    print(\"Non-skipped tokens only:\")\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44ef5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343\n",
      "9343\n",
      "{'precision': 0.3010989010989011, 'recall': 0.4228395061728395, 'f1': 0.35173299101412064, 'accuracy': 0.9459488387027721}\n",
      "Non-skipped tokens only:\n",
      "{'precision': 0.3010989010989011, 'recall': 0.4521452145214521, 'f1': 0.36147757255936674, 'accuracy': 0.9447362411509477}\n"
     ]
    }
   ],
   "source": [
    "if mistral03:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_id,\n",
    "    #     torch_dtype=\"auto\",\n",
    "    #     device_map=\"cuda\",\n",
    "    # )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=1024, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    # y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    # tokens = [t for (t                      , _) in pairs]\n",
    "    # gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    # n = min(len(tokens), len(y_pred))\n",
    "    # tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    # sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    # import pickle\n",
    "    # with open(paths.results/'Mistral-7B-Instruct-v0.3-dev.pkl', 'wb') as f:\n",
    "    #     pickle.dump((y_pred, skipped_jobs), f)\n",
    "    # import pickle\n",
    "    with open(paths.results/'Mistral-7B-Instruct-v0.3-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    # Create set of all token indices in skipped jobs\n",
    "    skipped_token_indices = set()\n",
    "    for job_idx in skipped_jobs:\n",
    "        job = jobs[job_idx]\n",
    "        start_token = job[\"start\"]\n",
    "        end_token = start_token + len(job[\"tokens\"])  # All tokens in this job\n",
    "        skipped_token_indices.update(range(start_token, end_token))\n",
    "\n",
    "    # Create new gold and pred lists excluding tokens from skipped jobs\n",
    "    new_gold = [label for idx, label in enumerate(gold) if idx not in skipped_token_indices]\n",
    "    new_y_pred = [pred for idx, pred in enumerate(y_pred) if idx not in skipped_token_indices]\n",
    "\n",
    "    # Second evaluation: only non-skipped tokens\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        new_gold, new_y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(new_gold, new_y_pred)\n",
    "    print(\"Non-skipped tokens only:\")\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f97c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if qwen2:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"Qwen/Qwen2-7B-Instruct\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=1024, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    tokens = [t for (t                      , _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'Qwen2-7B-Instruct-dev.pkl', 'wb') as f:\n",
    "        pickle.dump((y_pred, skipped_jobs), f)\n",
    "    import pickle\n",
    "    with open(paths.results/'Qwen2-7B-Instruct-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        remove_indices(gold, skipped_jobs), remove_indices(y_pred, skipped_jobs), labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80a04902",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 2 (255711868.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../results\")\n",
    "for y_pred, skipped_jobs in [\"Qwen2-7B-Instruct-dev.pkl\", 'Mistral-7B-Instruct-v0.3-dev.pkl', 'Mistral-7B-Instruct-v0.3-dev.pkl']:\n",
    "\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "gold = [y for (_, y) in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f694f86",
   "metadata": {},
   "source": [
    "# Minerva finetuning that doesnt work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_minerva7b:\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq, SPECIAL_MARKER\n",
    "    from minerva_lora import load_tokenizer_and_model\n",
    "\n",
    "    MINERVA7B = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
    "    bf16 = True\n",
    "    tokenizer, model = load_tokenizer_and_model(MINERVA7B, qlora=True, use_bf16=bf16)\n",
    "\n",
    "    import minerva_lora\n",
    "    import importlib\n",
    "    importlib.reload(minerva_lora)\n",
    "    from minerva_lora import build_examples_from_pairs, make_splits, lora_cfg\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_examples_from_pairs(pairs, 5, 1)\n",
    "    ds = make_splits(jobs, 0.1)\n",
    "    ds\n",
    "\n",
    "    from transformers import EarlyStoppingCallback   # NEW\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from peft import LoraConfig\n",
    "    import torch\n",
    "    import paths\n",
    "\n",
    "    # --- Training config ---\n",
    "    from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "\n",
    "    class ConsoleLogger(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if not logs: \n",
    "                return\n",
    "            # drop the huge/boring keys\n",
    "            drop = {\"total_flos\",\"train_runtime\",\"train_samples_per_second\",\"train_steps_per_second\"}\n",
    "            clean = {k: v for k, v in logs.items() if k not in drop}\n",
    "            print(f\"[step {state.global_step}/{state.max_steps}] {clean}\")\n",
    "\n",
    "    cfg = SFTConfig(\n",
    "        output_dir=paths.chekpoints/\"minerva\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=5,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        # <- logging every optimizer step\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        # <- eval + early stopping\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        # make stdout prints instead of only a tqdm bar:\n",
    "        disable_tqdm=True,\n",
    "        log_level=\"info\",\n",
    "        report_to=None,  # or \"none\"\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=True,\n",
    "        dataset_num_proc=2,\n",
    "        dataset_kwargs={\"prompt_column\":\"prompt\",\"completion_column\":\"completion\"},\n",
    "        completion_only_loss=True,\n",
    "    )\n",
    "\n",
    "    peft_config = lora_cfg()\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds.get(\"validation\"),\n",
    "        args=cfg,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0),\n",
    "            ConsoleLogger(),\n",
    "        ],\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Save PEFT adapters + tokenizer\n",
    "    trainer.model.save_pretrained(paths.chekpoints/\"minerva\")\n",
    "    tokenizer.save_pretrained(paths.chekpoints/\"minerva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "if eval_minerva7b:\n",
    "    import paths\n",
    "    from peft import PeftModel\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from gptoss_sent_split import (\n",
    "        SPECIAL_MARKER,          # \"<BOS>\"\n",
    "        SYSTEM_PROMPT,           # prompt with rules\n",
    "    )\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_dir = paths.chekpoints / \"minerva\"\n",
    "    base_model_name = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
    "\n",
    "    # 1. Load tokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "    # 2. Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # 3) If sizes differ, resize embeddings to tokenizer size (adds the new row)\n",
    "    if base_model.get_input_embeddings().weight.shape[0] != len(tok):\n",
    "        base_model.resize_token_embeddings(len(tok))\n",
    "        try:\n",
    "            base_model.tie_weights()   # safe if the model ties lm_head <-> embeddings\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3. Load LoRA adapters\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_dir)\n",
    "    model.eval(); model.config.use_cache = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e321c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import paths\n",
    "from peft import PeftModel\n",
    "\n",
    "import minerva_lora\n",
    "import importlib\n",
    "importlib.reload(minerva_lora)\n",
    "from minerva_lora import build_examples_from_pairs, make_splits, lora_cfg\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "jobs = build_examples_from_pairs(pairs, 5, 1)\n",
    "\n",
    "# inputs = tok(jobs[0]['prompt'], return_tensors=\"pt\").to(model.device)\n",
    "# out = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "# gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_markers(s: str) -> str:\n",
    "    # Accept a few variants just in case\n",
    "    s = re.sub(r\"<B(?:OS)?>\", \"<BOS>\", s)   # <B> or <BOS> -> <BOS>\n",
    "    # If you ever escaped them in HTML:\n",
    "    s = s.replace(\"&lt;BOS&gt;\", \"<BOS>\")\n",
    "    # Drop any stray repeated </s>\n",
    "    s = s.split(\"</s>\")[0]\n",
    "    return s\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing double quotes and backslashes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text with all double quotes and backslashes removed\n",
    "    \"\"\"\n",
    "    # Remove backslashes and double quotes\n",
    "    cleaned_text = text.replace('\\\\', '').replace('\"', '')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>Andando , guardava innanzi , ansioso insieme e timoroso di veder qualcheduno; e , dopo <BOS>pochi passi , vide infatti un uomo in camicia , seduto in terra , con le spalle appoggiate a una siepe di gelsomini , in un' attitudine d' insensato: e , a questa , e poi anche alla fisonomia , gli parve di raffigurar quel povero mezzo scemo di Gervaso ch' era venuto per secondo testimonio alla sciagurata spedizione. Ma es<BOS>endo gli si avvicinò , dovette accertarsi ch' era in vece quel Tonio così sveglio che ce l' aveva condotto. La peste , togliend<BOS>ogli il vigore del corpo insieme e della mente , gli aveva svolto in faccia e in ogni suo atto un piccolo e velato germe di somiglianza che aveva con l' incantato fratello. «Oh Tonio!» gli<BOS> disse Renzo , fermandosegli davanti: «sei tu?» Tonio alzò gli occhi , senza mover la testa.\n",
      "### System\n",
      "Rewrite the given text, inserting the token <BOS> before each sentence.\n",
      "Rules:\n",
      " - Keep ALL characters from the input unchanged.\n",
      " - Do not add or remove any characters other than inserting the marker.\n",
      " - Insert <BOS> immediately before the FIRST non-space character of each sentence.\n",
      " - A sentence ends with ., !, or ? (possibly followed by quotes or brackets).\n",
      "Output only the rewritten text.\n",
      "### User\n",
      "Tutt' a un tratto , sente uno squillo lontano , ma che gli par che venga dalle stanze , non dalla strada. Sta attento; lo sente più forte , più ripetuto , e insieme uno stropiccìo di piedi: un orrendo sospetto gli passa per la mente. Si rizza a sedere , e si mette ancor più attento; sente un rumor cupo nella stanza vicina , come d' un peso che venga messo giù con riguardo; butta le gambe fuor del letto , come per alzarsi , guarda all' uscio , lo vede aprirsi , vede presentarsi e venire avanti due logori e sudici vestiti rossi , due facce scomunicate , due monatti , in una parola; vede mezza la faccia del Griso che , nascosto dietro un battente socchiuso , riman lì a spiare. «Ah traditore infame!… Via , canaglia!\n",
      "### Assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(normalize_markers(gen)))\n",
    "print(clean_text(jobs[0]['prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30cafb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs[0]['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2220b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_special_tokens: ['<BOS>']\n",
      "'<BOS>' in vocab: True\n",
      "'<BOS>' pieces: ['<BOS>']\n",
      "'<BOS>' id: 51200\n"
     ]
    }
   ],
   "source": [
    "print(\"additional_special_tokens:\", tok.additional_special_tokens)\n",
    "print(\"'<BOS>' in vocab:\", \"<BOS>\" in tok.get_vocab())\n",
    "print(\"'<BOS>' pieces:\", tok.tokenize(\"<BOS>\"))\n",
    "print(\"'<BOS>' id:\", tok.convert_tokens_to_ids(\"<BOS>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
