{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3489e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch accelerate kernels\n",
    "# !pip install git+https://github.com/huggingface/transformers triton==3.4 git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "# !pip uninstall torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f4b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_train_set = False\n",
    "gpt_dev_set = True\n",
    "mistral01 = False\n",
    "mistral03 = False\n",
    "qwen2 = False\n",
    "classic = False\n",
    "\n",
    "\n",
    "train_minerva7b = False\n",
    "eval_minerva7b = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f602f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/paths.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import paths\n",
    "from huggingface_hub import login\n",
    "#Token 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\n",
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082d97ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7a233776ac411eb78e27af29cf6b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887299c31aed43db8d7101351168b8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Mxfp4Config\n",
    "if gpt_train_set or gpt_dev_set:\n",
    "\n",
    "    model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    quantization_config=Mxfp4Config.from_dict(config.quantization_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BOS labeling (pass 1):   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BOS labeling (pass 1): 100%|██████████| 1/1 [00:53<00:00, 53.78s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_map_bos_markers_to_sentence_starts\n",
      "\n",
      " analysisWe need to insert  before each sentence. Sentences end with ., !, or ?. Possibly followed by quotes or brackets. We must keep all characters unchanged. Insert  immediately before the first non-space character of each sentence. So we need to identify sentences. Input:\n",
      "\n",
      "\"In quanto a Don Gonzalo , poco dopo quella risposta , se n' andò da Milano; e la partenza fu trista per lui , come lo era la cagione. Veniva rimosso per i cattivi successi della guerra , della quale era stato il promotore e il capitano; e il popolo lo incolpava della fame sofferta sotto il suo governo. (Quello che aveva fatto per la peste , o non si sapeva , o certo nessuno se n' inquietava , come vedremo più avanti , fuorché il tribunale della sanità , e i due medici specialmente.)\"\n",
      "\n",
      "We need to treat each sentence. The first sentence ends at \"cagione.\" That's a period. So first sentence: \"In quanto a Don Gonzalo , poco dopo quella risposta , se n' andò da Milano; e la partenza fu trista per lui , come lo era la cagione.\" Insert  before first non-space char: \"In\". So \"In quanto a Don Gonzalo , poco dopo quella risposta , se n' andò da Milano; e la partenza fu trista per lui , come lo era la cagione.\"\n",
      "\n",
      "Second sentence: \"Veniva rimosso per i cattivi successi della guerra , della quale era stato il promotore e il capitano; e il popolo lo incolpava della fame sofferta sotto il suo governo.\" Ends with period. Insert  before \"Veniva\". So \"Veniva rimosso per i cattivi successi della guerra , della quale era stato il promotore e il capitano; e il popolo lo incolpava della fame sofferta sotto il suo governo.\"\n",
      "\n",
      "Third sentence: \"(Quello che aveva fatto per la peste , o non si sapeva , o certo nessuno se n' inquietava , come vedremo più avanti , fuorché il tribunale della sanità , e i due medici specialmente.)\" This is a sentence inside parentheses. It ends with a period before the closing parenthesis? Actually the period is inside parentheses: \"specialmente.)\" The period is before the closing parenthesis. So the sentence ends with period. Insert  before first non-space char: the first non-space char is \"(\". So we insert before \"(\". So \"(Quello che aveva fatto per la peste , o non si sapeva , o certo nessuno se n' inquietava , come vedremo più avanti , fuorché il tribunale della sanità , e i due medici specialmente.)\"\n",
      "\n",
      "We must keep all characters unchanged. So final output is the three sentences with  inserted. Ensure no extra spaces. Also note that the first sentence starts with \"In\" no leading spaces. So we add  before \"In\". Good.\n",
      "\n",
      "Let's produce final.assistantfinalIn quanto a Don Gonzalo , poco dopo quella risposta , se n' andò da Milano; e la partenza fu trista per lui , come lo era la cagione. Veniva rimosso per i cattivi successi della guerra , della quale era stato il promotore e il capitano; e il popolo lo incolpava della fame sofferta sotto il suo governo. (Quello che aveva fatto per la peste , o non si sapeva , o certo nessuno se n' inquietava , come vedremo più avanti , fuorché il tribunale della sanità , e i due medici specialmente.) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_map_bos_markers_to_sentence_starts\n",
      "\n",
      " In quanto a Don Gonzalo , poco dopo quella risposta , se n' andò da Milano; e la partenza fu trista per lui , come lo era la cagione. Veniva rimosso per i cattivi successi della guerra , della quale era stato il promotore e il capitano; e il popolo lo incolpava della fame sofferta sotto il suo governo. (Quello che aveva fatto per la peste , o non si sapeva , o certo nessuno se n' inquietava , come vedremo più avanti , fuorché il tribunale della sanità , e i due medici specialmente.) \n",
      "\n",
      "\n",
      "9343\n",
      "902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9343, 902]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gold))\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m prec, rec, f1, _ = \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbinary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m acc = accuracy_score(gold, y_pred)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(prec), \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rec), \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(f1), \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(acc)})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1996\u001b[39m, in \u001b[36mprecision_recall_fscore_support\u001b[39m\u001b[34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[39m\n\u001b[32m   1827\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[32m   1828\u001b[39m \n\u001b[32m   1829\u001b[39m \u001b[33;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1993\u001b[39m \u001b[33;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[32m   1994\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1995\u001b[39m _check_zero_division(zero_division)\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m labels = \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[32m   1999\u001b[39m samplewise = average == \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1762\u001b[39m, in \u001b[36m_check_set_wise_labels\u001b[39m\u001b[34m(y_true, y_pred, average, labels, pos_label)\u001b[39m\n\u001b[32m   1759\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33maverage has to be one of \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[32m   1761\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[32m   1764\u001b[39m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[32m   1765\u001b[39m present_labels = _tolist(unique_labels(y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:97\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m xp, _ = get_namespace(y_true, y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m type_true = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [9343, 902]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "import gptoss_sent_split\n",
    "importlib.reload(gptoss_sent_split)\n",
    "from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "if gpt_dev_set:\n",
    "\n",
    "    cfg = BOSConfig(max_new_tokens=10000, n_sentences=3, batch_size=16)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg, model_id=model_id)\n",
    "\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'gptpredval.pkl', 'wb') as f:\n",
    "        pickle.dump((y_pred, skipped_jobs), f)\n",
    "with open(paths.results/'gptpredval.pkl', 'rb') as f:\n",
    "    y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "gold = [y for (_, y) in pairs]\n",
    "print(len(gold))\n",
    "print(len(y_pred))\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    ")\n",
    "acc = accuracy_score(gold, y_pred)\n",
    "print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "# Create set of all token indices in skipped jobs\n",
    "skipped_token_indices = set()\n",
    "for job_idx in skipped_jobs:\n",
    "    job = jobs[job_idx]\n",
    "    start_token = job[\"start\"]\n",
    "    end_token = start_token + len(job[\"tokens\"])  # All tokens in this job\n",
    "    skipped_token_indices.update(range(start_token, end_token))\n",
    "\n",
    "# Create new gold and pred lists excluding tokens from skipped jobs\n",
    "new_gold = [label for idx, label in enumerate(gold) if idx not in skipped_token_indices]\n",
    "new_y_pred = [pred for idx, pred in enumerate(y_pred) if idx not in skipped_token_indices]\n",
    "\n",
    "# Second evaluation: only non-skipped tokens\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    new_gold, new_y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    ")\n",
    "acc = accuracy_score(new_gold, new_y_pred)\n",
    "print(\"Non-skipped tokens only:\")\n",
    "print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d2af0",
   "metadata": {},
   "source": [
    "# Similar models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mistral01:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_id,\n",
    "    #     torch_dtype=\"auto\",\n",
    "    #     device_map=\"cuda\",\n",
    "    # )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=512, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    # y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    # tokens = [t for (t                      , _) in pairs]\n",
    "    # gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # # Align lengths, just in case\n",
    "    # n = min(len(tokens), len(y_pred))\n",
    "    # tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    # sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    # import pickle\n",
    "    # with open(paths.results/'Mistral-7B-Instruct-v0.1-dev.pkl', 'wb') as f:\n",
    "    #     pickle.dump((y_pred, skipped_jobs), f)\n",
    "    # import pickle\n",
    "    with open(paths.results/'Mistral-7B-Instruct-v0.1-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    # Create set of all token indices in skipped jobs\n",
    "    skipped_token_indices = set()\n",
    "    for job_idx in skipped_jobs:\n",
    "        job = jobs[job_idx]\n",
    "        start_token = job[\"start\"]\n",
    "        end_token = start_token + len(job[\"tokens\"])  # All tokens in this job\n",
    "        skipped_token_indices.update(range(start_token, end_token))\n",
    "\n",
    "    # Create new gold and pred lists excluding tokens from skipped jobs\n",
    "    new_gold = [label for idx, label in enumerate(gold) if idx not in skipped_token_indices]\n",
    "    new_y_pred = [pred for idx, pred in enumerate(y_pred) if idx not in skipped_token_indices]\n",
    "\n",
    "    # Second evaluation: only non-skipped tokens\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        new_gold, new_y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(new_gold, new_y_pred)\n",
    "    print(\"Non-skipped tokens only:\")\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343\n",
      "9343\n",
      "{'precision': 0.3010989010989011, 'recall': 0.4228395061728395, 'f1': 0.35173299101412064, 'accuracy': 0.9459488387027721}\n",
      "Non-skipped tokens only:\n",
      "{'precision': 0.3010989010989011, 'recall': 0.4521452145214521, 'f1': 0.36147757255936674, 'accuracy': 0.9447362411509477}\n"
     ]
    }
   ],
   "source": [
    "if mistral03:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_id,\n",
    "    #     torch_dtype=\"auto\",\n",
    "    #     device_map=\"cuda\",\n",
    "    # )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=1024, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    # y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    # tokens = [t for (t                      , _) in pairs]\n",
    "    # gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    # n = min(len(tokens), len(y_pred))\n",
    "    # tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    # sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    # import pickle\n",
    "    # with open(paths.results/'Mistral-7B-Instruct-v0.3-dev.pkl', 'wb') as f:\n",
    "    #     pickle.dump((y_pred, skipped_jobs), f)\n",
    "    # import pickle\n",
    "    with open(paths.results/'Mistral-7B-Instruct-v0.3-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    # Create set of all token indices in skipped jobs\n",
    "    skipped_token_indices = set()\n",
    "    for job_idx in skipped_jobs:\n",
    "        job = jobs[job_idx]\n",
    "        start_token = job[\"start\"]\n",
    "        end_token = start_token + len(job[\"tokens\"])  # All tokens in this job\n",
    "        skipped_token_indices.update(range(start_token, end_token))\n",
    "\n",
    "    # Create new gold and pred lists excluding tokens from skipped jobs\n",
    "    new_gold = [label for idx, label in enumerate(gold) if idx not in skipped_token_indices]\n",
    "    new_y_pred = [pred for idx, pred in enumerate(y_pred) if idx not in skipped_token_indices]\n",
    "\n",
    "    # Second evaluation: only non-skipped tokens\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        new_gold, new_y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(new_gold, new_y_pred)\n",
    "    print(\"Non-skipped tokens only:\")\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f97c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if qwen2:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import sys\n",
    "    sys.path.append('../src')\n",
    "    import paths\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    def remove_indices(data, indices_to_remove):\n",
    "        result = [item for idx, item in enumerate(data) if idx not in indices_to_remove]\n",
    "        return result\n",
    "\n",
    "    # Choose any compatible model from above\n",
    "    model_id = \"Qwen/Qwen2-7B-Instruct\"  # Example\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "\n",
    "    # Your existing code will work the same way\n",
    "    cfg = BOSConfig(max_new_tokens=1024, n_sentences=3)\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred, skipped_jobs = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "    tokens = [t for (t                      , _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'Qwen2-7B-Instruct-dev.pkl', 'wb') as f:\n",
    "        pickle.dump((y_pred, skipped_jobs), f)\n",
    "    import pickle\n",
    "    with open(paths.results/'Qwen2-7B-Instruct-dev.pkl', 'rb') as f:\n",
    "        y_pred, skipped_jobs = pickle.load(f)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    gold = [y for (_, y) in pairs]\n",
    "    print(len(gold))\n",
    "    print(len(y_pred))\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        remove_indices(gold, skipped_jobs), remove_indices(y_pred, skipped_jobs), labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13c6c0",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3158f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9343 9343\n",
      "{'precision': 0.6465968586387435, 'recall': 0.7623456790123457, 'f1': 0.6997167138810199, 'accuracy': 0.9773092154554212}\n"
     ]
    }
   ],
   "source": [
    "if classic:\n",
    "    # Fallback-only sentence boundary baseline (no model calls)\n",
    "    import importlib, pickle\n",
    "    import dataset\n",
    "    importlib.reload(dataset)\n",
    "    importlib.reload(gss)\n",
    "\n",
    "    from gptoss_sent_split import read_token_label_file, _fallback_punct_labels, sentences_from_word_seq\n",
    "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "    # Load tokens/gold labels\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold   = [y for (_, y) in pairs]\n",
    "\n",
    "    # Predict with the simple punctuation heuristic\n",
    "    y_pred = _fallback_punct_labels(tokens)\n",
    "\n",
    "    # (optional) save predictions for later comparison\n",
    "    with open(paths.results/'punctpredval.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "\n",
    "    # Metrics\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        gold, y_pred, labels=[1], average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(gold, y_pred)\n",
    "\n",
    "    print(len(gold), len(y_pred))\n",
    "    print({\"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1), \"accuracy\": float(acc)})\n",
    "\n",
    "    # (optional) reconstruct predicted sentences\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    # 'sents' is a list of token lists; use as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f694f86",
   "metadata": {},
   "source": [
    "# Minerva finetuning that doesnt work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_minerva7b:\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq, SPECIAL_MARKER\n",
    "    from minerva_lora import load_tokenizer_and_model\n",
    "\n",
    "    MINERVA7B = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
    "    bf16 = True\n",
    "    tokenizer, model = load_tokenizer_and_model(MINERVA7B, qlora=True, use_bf16=bf16)\n",
    "\n",
    "    import minerva_lora\n",
    "    import importlib\n",
    "    importlib.reload(minerva_lora)\n",
    "    from minerva_lora import build_examples_from_pairs, make_splits, lora_cfg\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_examples_from_pairs(pairs, 5, 1)\n",
    "    ds = make_splits(jobs, 0.1)\n",
    "    ds\n",
    "\n",
    "    from transformers import EarlyStoppingCallback   # NEW\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from peft import LoraConfig\n",
    "    import torch\n",
    "    import paths\n",
    "\n",
    "    # --- Training config ---\n",
    "    from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "\n",
    "    class ConsoleLogger(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if not logs: \n",
    "                return\n",
    "            # drop the huge/boring keys\n",
    "            drop = {\"total_flos\",\"train_runtime\",\"train_samples_per_second\",\"train_steps_per_second\"}\n",
    "            clean = {k: v for k, v in logs.items() if k not in drop}\n",
    "            print(f\"[step {state.global_step}/{state.max_steps}] {clean}\")\n",
    "\n",
    "    cfg = SFTConfig(\n",
    "        output_dir=paths.chekpoints/\"minerva\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=5,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        # <- logging every optimizer step\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1,\n",
    "        logging_first_step=True,\n",
    "        # <- eval + early stopping\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        # make stdout prints instead of only a tqdm bar:\n",
    "        disable_tqdm=True,\n",
    "        log_level=\"info\",\n",
    "        report_to=None,  # or \"none\"\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=True,\n",
    "        dataset_num_proc=2,\n",
    "        dataset_kwargs={\"prompt_column\":\"prompt\",\"completion_column\":\"completion\"},\n",
    "        completion_only_loss=True,\n",
    "    )\n",
    "\n",
    "    peft_config = lora_cfg()\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds.get(\"validation\"),\n",
    "        args=cfg,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0),\n",
    "            ConsoleLogger(),\n",
    "        ],\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Save PEFT adapters + tokenizer\n",
    "    trainer.model.save_pretrained(paths.chekpoints/\"minerva\")\n",
    "    tokenizer.save_pretrained(paths.chekpoints/\"minerva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "if eval_minerva7b:\n",
    "    import paths\n",
    "    from peft import PeftModel\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from gptoss_sent_split import (\n",
    "        SPECIAL_MARKER,          # \"<BOS>\"\n",
    "        SYSTEM_PROMPT,           # prompt with rules\n",
    "    )\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_dir = paths.chekpoints / \"minerva\"\n",
    "    base_model_name = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
    "\n",
    "    # 1. Load tokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "    # 2. Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # 3) If sizes differ, resize embeddings to tokenizer size (adds the new row)\n",
    "    if base_model.get_input_embeddings().weight.shape[0] != len(tok):\n",
    "        base_model.resize_token_embeddings(len(tok))\n",
    "        try:\n",
    "            base_model.tie_weights()   # safe if the model ties lm_head <-> embeddings\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3. Load LoRA adapters\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_dir)\n",
    "    model.eval(); model.config.use_cache = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e321c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import paths\n",
    "from peft import PeftModel\n",
    "\n",
    "import minerva_lora\n",
    "import importlib\n",
    "importlib.reload(minerva_lora)\n",
    "from minerva_lora import build_examples_from_pairs, make_splits, lora_cfg\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "jobs = build_examples_from_pairs(pairs, 5, 1)\n",
    "\n",
    "# inputs = tok(jobs[0]['prompt'], return_tensors=\"pt\").to(model.device)\n",
    "# out = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "# gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_markers(s: str) -> str:\n",
    "    # Accept a few variants just in case\n",
    "    s = re.sub(r\"<B(?:OS)?>\", \"<BOS>\", s)   # <B> or <BOS> -> <BOS>\n",
    "    # If you ever escaped them in HTML:\n",
    "    s = s.replace(\"&lt;BOS&gt;\", \"<BOS>\")\n",
    "    # Drop any stray repeated </s>\n",
    "    s = s.split(\"</s>\")[0]\n",
    "    return s\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing double quotes and backslashes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text with all double quotes and backslashes removed\n",
    "    \"\"\"\n",
    "    # Remove backslashes and double quotes\n",
    "    cleaned_text = text.replace('\\\\', '').replace('\"', '')\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>Andando , guardava innanzi , ansioso insieme e timoroso di veder qualcheduno; e , dopo <BOS>pochi passi , vide infatti un uomo in camicia , seduto in terra , con le spalle appoggiate a una siepe di gelsomini , in un' attitudine d' insensato: e , a questa , e poi anche alla fisonomia , gli parve di raffigurar quel povero mezzo scemo di Gervaso ch' era venuto per secondo testimonio alla sciagurata spedizione. Ma es<BOS>endo gli si avvicinò , dovette accertarsi ch' era in vece quel Tonio così sveglio che ce l' aveva condotto. La peste , togliend<BOS>ogli il vigore del corpo insieme e della mente , gli aveva svolto in faccia e in ogni suo atto un piccolo e velato germe di somiglianza che aveva con l' incantato fratello. «Oh Tonio!» gli<BOS> disse Renzo , fermandosegli davanti: «sei tu?» Tonio alzò gli occhi , senza mover la testa.\n",
      "### System\n",
      "Rewrite the given text, inserting the token <BOS> before each sentence.\n",
      "Rules:\n",
      " - Keep ALL characters from the input unchanged.\n",
      " - Do not add or remove any characters other than inserting the marker.\n",
      " - Insert <BOS> immediately before the FIRST non-space character of each sentence.\n",
      " - A sentence ends with ., !, or ? (possibly followed by quotes or brackets).\n",
      "Output only the rewritten text.\n",
      "### User\n",
      "Tutt' a un tratto , sente uno squillo lontano , ma che gli par che venga dalle stanze , non dalla strada. Sta attento; lo sente più forte , più ripetuto , e insieme uno stropiccìo di piedi: un orrendo sospetto gli passa per la mente. Si rizza a sedere , e si mette ancor più attento; sente un rumor cupo nella stanza vicina , come d' un peso che venga messo giù con riguardo; butta le gambe fuor del letto , come per alzarsi , guarda all' uscio , lo vede aprirsi , vede presentarsi e venire avanti due logori e sudici vestiti rossi , due facce scomunicate , due monatti , in una parola; vede mezza la faccia del Griso che , nascosto dietro un battente socchiuso , riman lì a spiare. «Ah traditore infame!… Via , canaglia!\n",
      "### Assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(normalize_markers(gen)))\n",
    "print(clean_text(jobs[0]['prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30cafb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs[0]['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2220b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_special_tokens: ['<BOS>']\n",
      "'<BOS>' in vocab: True\n",
      "'<BOS>' pieces: ['<BOS>']\n",
      "'<BOS>' id: 51200\n"
     ]
    }
   ],
   "source": [
    "print(\"additional_special_tokens:\", tok.additional_special_tokens)\n",
    "print(\"'<BOS>' in vocab:\", \"<BOS>\" in tok.get_vocab())\n",
    "print(\"'<BOS>' pieces:\", tok.tokenize(\"<BOS>\"))\n",
    "print(\"'<BOS>' id:\", tok.convert_tokens_to_ids(\"<BOS>\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
