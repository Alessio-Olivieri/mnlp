{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3489e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade torch accelerate kernels\n",
    "# !pip install -q git+https://github.com/huggingface/transformers triton==3.4 git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "# !pip uninstall -q torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f4b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f602f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/paths.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082d97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpt:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, Mxfp4Config\n",
    "\n",
    "    model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    quantization_config=Mxfp4Config.from_dict(config.quantization_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb7164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpt:\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    cfg = BOSConfig(max_new_tokens=256)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_train_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'gptpredtrain.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0974929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpt:\n",
    "    import pandas as pd\n",
    "    import dataset\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    import gptoss_sent_split\n",
    "    importlib.reload(gptoss_sent_split)\n",
    "    from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq\n",
    "\n",
    "    cfg = BOSConfig(max_new_tokens=256)\n",
    "\n",
    "    pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "    jobs = build_bos_jobs_by_n_sentences(pairs, tokenizer, cfg)\n",
    "    y_pred = run_bos_labeling(jobs, model, tokenizer, cfg)\n",
    "\n",
    "    tokens = [t for (t, _) in pairs]\n",
    "    gold = [y for (_, y) in pairs]\n",
    "\n",
    "    # Align lengths, just in case\n",
    "    n = min(len(tokens), len(y_pred))\n",
    "    tokens, gold, y_pred = tokens[:n], gold[:n], y_pred[:n]\n",
    "    sents = sentences_from_word_seq(tokens, y_pred)\n",
    "    import pickle\n",
    "    with open(paths.results/'gptpredval.pkl', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3cb1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.08s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from gptoss_sent_split import BOSConfig, read_token_label_file, build_bos_jobs_by_n_sentences, run_bos_labeling, sentences_from_word_seq, SPECIAL_MARKER\n",
    "from minerva_lora import load_tokenizer_and_model\n",
    "\n",
    "MINERVA7B = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
    "bf16 = True\n",
    "tokenizer, model = load_tokenizer_and_model(MINERVA7B, qlora=True, use_bf16=bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351ae219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 73\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minerva_lora\n",
    "import importlib\n",
    "importlib.reload(minerva_lora)\n",
    "from minerva_lora import build_examples_from_pairs, make_splits, lora_cfg\n",
    "\n",
    "pairs = read_token_label_file(paths.data/\"manzoni_dev_tokens.csv\")\n",
    "jobs = build_examples_from_pairs(pairs, 5, 1)\n",
    "ds = make_splits(jobs, 0.1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb41286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/.venv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/user/mnlp/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset (num_proc=2): 100%|██████████| 73/73 [00:00<00:00, 215.51 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|██████████| 73/73 [00:00<00:00, 117.51 examples/s]\n",
      "Truncating train dataset (num_proc=2): 100%|██████████| 73/73 [00:00<00:00, 189.94 examples/s]\n",
      "Adding EOS to eval dataset (num_proc=2): 100%|██████████| 8/8 [00:00<00:00, 22.73 examples/s]\n",
      "Tokenizing eval dataset (num_proc=2): 100%|██████████| 8/8 [00:00<00:00, 14.11 examples/s]\n",
      "Truncating eval dataset (num_proc=2): 100%|██████████| 8/8 [00:00<00:00, 21.88 examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     19\u001b[39m cfg = SFTConfig(\n\u001b[32m     20\u001b[39m     output_dir=paths.chekpoints/\u001b[33m\"\u001b[39m\u001b[33mminerva\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     num_train_epochs=\u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     completion_only_loss=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     46\u001b[39m )\n\u001b[32m     48\u001b[39m peft_config = lora_cfg()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEarlyStoppingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mConsoleLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m trainer.train()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Save PEFT adapters + tokenizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:544\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m._total_train_tokens = \u001b[32m0\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Initialize the Trainer. Parent class will handle:\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# - DeepSpeed configuration (through create_accelerator_and_postprocess)\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# - FSDP setup\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# - Distributed training setup\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m# - Optimizer and scheduler creation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Initialize activation offloading context\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.activation_offloading:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:688\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    686\u001b[39m default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(\u001b[38;5;28mself\u001b[39m.args.report_to)\n\u001b[32m    687\u001b[39m callbacks = default_callbacks \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_callbacks + callbacks\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m \u001b[38;5;28mself\u001b[39m.callback_handler = \u001b[43mCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m.add_callback(PrinterCallback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.disable_tqdm \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer_callback.py:449\u001b[39m, in \u001b[36mCallbackHandler.__init__\u001b[39m\u001b[34m(self, callbacks, model, processing_class, optimizer, lr_scheduler)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28mself\u001b[39m.callbacks = []\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m.processing_class = processing_class\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer_callback.py:466\u001b[39m, in \u001b[36mCallbackHandler.add_callback\u001b[39m\u001b[34m(self, callback)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback):\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     cb = \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback\n\u001b[32m    467\u001b[39m     cb_class = callback \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callback, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m callback.\u001b[34m__class__\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cb_class \u001b[38;5;129;01min\u001b[39;00m [c.\u001b[34m__class__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py:680\u001b[39m, in \u001b[36mTensorBoardCallback.__init__\u001b[39m\u001b[34m(self, tb_writer)\u001b[39m\n\u001b[32m    678\u001b[39m has_tensorboard = is_tensorboard_available()\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tensorboard:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m install tensorboardX.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    683\u001b[39m     )\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_tensorboard:\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX."
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback   # NEW\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import paths\n",
    "\n",
    "# --- Training config ---\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "\n",
    "class ConsoleLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs: \n",
    "            return\n",
    "        # drop the huge/boring keys\n",
    "        drop = {\"total_flos\",\"train_runtime\",\"train_samples_per_second\",\"train_steps_per_second\"}\n",
    "        clean = {k: v for k, v in logs.items() if k not in drop}\n",
    "        print(f\"[step {state.global_step}/{state.max_steps}] {clean}\")\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=paths.chekpoints/\"minerva\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    # <- logging every optimizer step\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    # <- eval + early stopping\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    # make stdout prints instead of only a tqdm bar:\n",
    "    disable_tqdm=True,\n",
    "    log_level=\"info\",\n",
    "    report_to=None,  # or \"none\"\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    dataset_num_proc=2,\n",
    "    dataset_kwargs={\"prompt_column\":\"prompt\",\"completion_column\":\"completion\"},\n",
    "    completion_only_loss=True,\n",
    ")\n",
    "\n",
    "peft_config = lora_cfg()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds.get(\"validation\"),\n",
    "    args=cfg,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0),\n",
    "        ConsoleLogger(),\n",
    "    ],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save PEFT adapters + tokenizer\n",
    "trainer.model.save_pretrained(paths.chekpoints/\"minerva\")\n",
    "tokenizer.save_pretrained(paths.chekpoints/\"minerva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9f86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### Quick inference check with the 7B LoRA adapter\n",
    "\n",
    "import transformers, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from gptoss_sent_split import SPECIAL_MARKER, SYSTEM_PROMPT\n",
    "\n",
    "base_id = MINERVA7B\n",
    "adapter_dir = \"/home/user/mnlp/checkpoints/minerva\"\n",
    "\n",
    "# 1) Load tokenizer that includes <BOS> (saved during training)\n",
    "tok = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n",
    "\n",
    "# 2) Load base model\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                             bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_id,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) If sizes differ, resize embeddings to tokenizer size (adds the new row)\n",
    "if base.get_input_embeddings().weight.shape[0] != len(tok):\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    try:\n",
    "        base.tie_weights()   # safe if the model ties lm_head <-> embeddings\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# 4) Now load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(base, adapter_dir)\n",
    "model.eval()\n",
    "\n",
    "def generate_bos(text: str, max_new_tokens: int = 2048):\n",
    "    prompt = (\n",
    "        \"### System\\n\" + SYSTEM_PROMPT + \"\\n\"\n",
    "        \"### User\\n\" + text + \"\\n\"\n",
    "        \"### Assistant\\n\"\n",
    "    )\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=2048, do_sample=False)\n",
    "    gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebefcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<B>Ci vuol degli uomini fatti apposta.» Altre volte Renzo si risolveva d' andar di nascosto \",\" travestito \",\" e con un nome finto. Ma anche da qu<B>esto \",\" Bortolo seppe svolgerlo ogni volta \",\" con ragioni troppo facili a indovinarsi. Scoppiata poi la peste<B> nel milanese \",\" e appunto \",\" come abbiam detto \",\" sul confine del bergamasco \",\" non tardò molto a passarlo; e... non vi sgomentate \",\" ch' io non vi voglio raccontar la storia anche di questa: chi la\n"
     ]
    }
   ],
   "source": [
    "def generate_bos(text: str, max_new_tokens: int = 2048):\n",
    "    prompt = (\n",
    "        \"### System\\n\" + SYSTEM_PROMPT + \"\\n\"\n",
    "        \"### User\\n\" + text + \"\\n\"\n",
    "        \"### Assistant\\n\"\n",
    "    )\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=2048, do_sample=False)\n",
    "    gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return gen\n",
    "\n",
    "sample = 'Ci vuol degli uomini fatti apposta.» Altre volte Renzo si risolveva d\\' andar di nascosto \",\" travestito \",\" e con un nome finto. Ma anche da questo \",\" Bortolo seppe svolgerlo ogni volta \",\" con ragioni troppo facili a indovinarsi. Scoppiata poi la peste nel milanese \",\" e appunto \",\" come abbiam detto \",\" sul confine del bergamasco \",\" non tardò molto a passarlo; e… non vi sgomentate \",\" ch\\' io non vi voglio raccontar la storia anche di questa: chi la volesse \",\" la c\\' è \",\" scritta per ordine pubblico da un certo Lorenzo Ghirardelli: libro raro però e sconosciuto \",\" quantunque contenga forse più roba che tutte insieme le descrizioni più celebri di pestilenze: da tante cose dipende la celebrità de\\' libri! Quel ch\\' io volevo dire è che Renzo prese anche lui la peste \",\" si curò da sé \",\" cioè non fece nulla; ne fu in fin di morte \",\" ma la sua buona complessione vinse la forza del male: in pochi giorni \",\" si trovò fuor di pericolo.'\n",
    "print(generate_bos(sample))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
