{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import paths\n",
    "import dataset\n",
    "import train\n",
    "import utils\n",
    "import torch\n",
    "import pickle\n",
    "import evaluation\n",
    "\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7feacc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fa9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = paths.data/\"manzoni_train_tokens.csv\"\n",
    "OOD_DATA = paths.data/\"manzoni_dev_tokens.csv\"\n",
    "HF_DATA = paths.data/\"prepared\"\n",
    "torch.set_float32_matmul_precision(\"high\")   # enable TF32 matmuls on Ampere\n",
    "torch.backends.cudnn.allow_tf32 = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f383f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def check_labels(hfds_split, sample_rows=2000):\n",
    "    # Concatenate labels from a subset of rows (pad to same length already handled by collator)\n",
    "    n = min(sample_rows, len(hfds_split))\n",
    "    cats = []\n",
    "    for ex in hfds_split.select(range(n)):\n",
    "        labs = np.array(ex[\"labels\"])\n",
    "        cats.append(labs)\n",
    "    all_labs = np.concatenate(cats)\n",
    "    visible = all_labs[all_labs != -100]\n",
    "    uniq = np.unique(visible)\n",
    "    print(\"Unique visible labels:\", uniq)\n",
    "    bad = [x for x in uniq if x not in (0, 1)]\n",
    "    if bad:\n",
    "        print(\"❌ Found out-of-range labels:\", bad)\n",
    "    else:\n",
    "        print(\"✅ Labels look fine (only 0/1).\")\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12bed86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/deberta-v3-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2498ab094f40ae8b62c1b1d5167e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bffe7e2d864fcab1b86fffbed90052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907e47216f6b4afd9958a0a7143dd0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542e4d8a97eb44fe98ec9dca639122e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d23192ce78467f81b0b36f4039598e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/298 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41afcf5426114f8d94c2a34d48c848b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc656a08d663464eb0c989ba71e6b8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "\n",
      "=== Training deberta -> microsoft/deberta-v3-base ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7012555dda6148c28797f54ea9e32267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5581e1e2d3e1459b97d008e369e2cd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mnlp/notebooks/../src/train.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 00:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.985401</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.998844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.999653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.998168</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.996344</td>\n",
       "      <td>0.999769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deberta] Validation: {'eval_loss': 0.0010188599117100239, 'eval_precision': 0.9981684981684982, 'eval_recall': 0.9945255474452555, 'eval_f1': 0.9963436928702011, 'eval_accuracy': 0.9997688529326784, 'eval_runtime': 0.5369, 'eval_samples_per_second': 111.753, 'eval_steps_per_second': 14.9, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answerdotai/ModernBERT-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78df2627cd43aebcd66a1ea8f65b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f5ba154b9b4dcbae5b1b66cbff1c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf5f95ebc84109ac00895564facbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f19006b2d433d89acdc011bf16f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d298725895f4eb9ac814bb98d08c8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a513e5a92647e893a3688574501b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2927cc5fabd74b60b40fa635fb8c96a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "\n",
      "=== Training modernbert -> answerdotai/ModernBERT-base ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c513f2ad704c48b389b2f58396edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126b69ec133b4e3e9bfc7512e701f948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mnlp/notebooks/../src/train.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.064170</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.419408</td>\n",
       "      <td>0.567297</td>\n",
       "      <td>0.976423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.924125</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.846702</td>\n",
       "      <td>0.989575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.951827</td>\n",
       "      <td>0.942434</td>\n",
       "      <td>0.947107</td>\n",
       "      <td>0.996121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modernbert] Validation: {'eval_loss': 0.01282982062548399, 'eval_precision': 0.9518272425249169, 'eval_recall': 0.9424342105263158, 'eval_f1': 0.947107438016529, 'eval_accuracy': 0.9961209770289109, 'eval_runtime': 0.3558, 'eval_samples_per_second': 39.345, 'eval_steps_per_second': 5.621, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752a9d0b30e46fd8222208312501355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558d8a3c0bca4c3282e08c95b4389943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16801c2eba0649f4a8b993d2ba16630d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45ba36ecad94da78042e00d139e74ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ec464da1e648d3831f3f03e1f6c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ed536009cd4f1aaa4080bb9b157f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edbc5e6809c4293952608cb3c977f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818f3ab496df484698850a3247505766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/53 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "Unique visible labels: [0 1]\n",
      "✅ Labels look fine (only 0/1).\n",
      "\n",
      "=== Training bert -> bert-base-multilingual-cased ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f83f6952ed4932999bfa0af5bb7e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/mnlp/notebooks/../src/train.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.017423</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>0.914106</td>\n",
       "      <td>0.993690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.954098</td>\n",
       "      <td>0.996575</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>0.998263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.989726</td>\n",
       "      <td>0.989726</td>\n",
       "      <td>0.989726</td>\n",
       "      <td>0.999305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bert] Validation: {'eval_loss': 0.0031423659529536963, 'eval_precision': 0.9897260273972602, 'eval_recall': 0.9897260273972602, 'eval_f1': 0.9897260273972602, 'eval_accuracy': 0.9993053143452588, 'eval_runtime': 0.2373, 'eval_samples_per_second': 223.383, 'eval_steps_per_second': 29.503, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deberta': {'eval_loss': 0.0010188599117100239, 'eval_precision': 0.9981684981684982, 'eval_recall': 0.9945255474452555, 'eval_f1': 0.9963436928702011, 'eval_accuracy': 0.9997688529326784, 'eval_runtime': 0.5369, 'eval_samples_per_second': 111.753, 'eval_steps_per_second': 14.9, 'epoch': 3.0}, 'modernbert': {'eval_loss': 0.01282982062548399, 'eval_precision': 0.9518272425249169, 'eval_recall': 0.9424342105263158, 'eval_f1': 0.947107438016529, 'eval_accuracy': 0.9961209770289109, 'eval_runtime': 0.3558, 'eval_samples_per_second': 39.345, 'eval_steps_per_second': 5.621, 'epoch': 3.0}, 'bert': {'eval_loss': 0.0031423659529536963, 'eval_precision': 0.9897260273972602, 'eval_recall': 0.9897260273972602, 'eval_f1': 0.9897260273972602, 'eval_accuracy': 0.9993053143452588, 'eval_runtime': 0.2373, 'eval_samples_per_second': 223.383, 'eval_steps_per_second': 29.503, 'epoch': 3.0}}\n"
     ]
    }
   ],
   "source": [
    "if train_flag:\n",
    "    import importlib\n",
    "    importlib.reload(dataset)\n",
    "    importlib.reload(train)\n",
    "\n",
    "    import os\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # makes the exception point to the correct op\n",
    "\n",
    "    results = {}\n",
    "    for model_key in [\"deberta\", \"modernbert\", \"bert\"]:\n",
    "        pairs = dataset.read_token_label_file(TRAIN_DATA)\n",
    "        sents_tok, sents_lab = dataset.group_into_sentences(pairs)\n",
    "        ds_full = dataset.build_hf_dataset_for_token_classification(sents_tok, sents_lab, model_key=model_key)\n",
    "        split = ds_full.train_test_split(train_size=0.8, seed=69)\n",
    "        train_ds = dataset.tidy(split[\"train\"], model_key)\n",
    "        val_ds   = dataset.tidy(split[\"test\"], model_key)\n",
    "        train_ds.save_to_disk(HF_DATA/f\"{model_key}\"/\"train\")\n",
    "        val_ds.save_to_disk(HF_DATA/f\"{model_key}\"/\"val\")\n",
    "        _ = check_labels(train_ds)  # your ModernBERT train split\n",
    "        _ = check_labels(val_ds)\n",
    "        print(f\"\\n=== Training {model_key} -> {utils.MODEL_SPECS[model_key].name} ===\")\n",
    "        out_dir = str(paths.chekpoints / model_key)\n",
    "        results[model_key] = train.train_token_splitter(\n",
    "            train_ds, val_ds,\n",
    "            model_key=model_key, out_dir=out_dir,\n",
    "            lr=5e-5, batch_size=8, epochs=3,\n",
    "        )\n",
    "    with open(paths.results/\"token_class_eval.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "else:\n",
    "    with open(paths.results/\"token_class_eval.pkl\", \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f14dfa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'eval_f1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_20326/3897511101.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m pandas \u001b[38;5;28;01mas\u001b[39;00m pd\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pd.DataFrame(results).T.sort_values(\u001b[33m\"eval_f1\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7194\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7195\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7196\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7197\u001b[39m \n\u001b[32m   7198\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7199\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_f1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(results).T.sort_values(\"eval_f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4817667a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m      2\u001b[39m importlib.reload(evaluation)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_key = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_f1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model_dir = paths.chekpoints/best_key\n\u001b[32m      5\u001b[39m best_trainer = evaluation.load_trainer_for_eval(model_dir, HF_DATA/best_key/\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m      2\u001b[39m importlib.reload(evaluation)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_key = \u001b[38;5;28mmax\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m k: \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_f1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      4\u001b[39m model_dir = paths.chekpoints/best_key\n\u001b[32m      5\u001b[39m best_trainer = evaluation.load_trainer_for_eval(model_dir, HF_DATA/best_key/\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(evaluation)\n",
    "best_key = max(results, key=lambda k: results[k][\"eval_f1\"])\n",
    "model_dir = paths.chekpoints/best_key\n",
    "best_trainer = evaluation.load_trainer_for_eval(model_dir, HF_DATA/best_key/\"val\")\n",
    "val_ds = load_from_disk(HF_DATA/best_key/\"val\")\n",
    "\n",
    "pred = best_trainer.predict(val_ds)  # logits + label_ids as np arrays\n",
    "logits = pred.predictions\n",
    "label_ids = pred.label_ids\n",
    "tok = best_trainer.tokenizer\n",
    "\n",
    "def sentences_from_word_seq(words, y_pred):\n",
    "    sents, cur = [], []\n",
    "    for w, b in zip(words, y_pred):\n",
    "        cur.append(w)\n",
    "        if b == 1:\n",
    "            sents.append(cur); cur = []\n",
    "    if cur: sents.append(cur)\n",
    "    return sents\n",
    "\n",
    "for i in range(min(3, len(val_ds))):\n",
    "    ids = val_ds[i][\"input_ids\"]\n",
    "    words = tok.convert_ids_to_tokens(ids)\n",
    "\n",
    "    mask = (label_ids[i] != -100)          # np.bool_ array\n",
    "    y_pred = logits[i].argmax(-1)[mask]    # predicted boundary labels at visible positions\n",
    "    visible_words = [w for w, m in zip(words, mask.tolist()) if m]\n",
    "\n",
    "    sents = sentences_from_word_seq(visible_words, y_pred)\n",
    "    print(f\"\\nWindow {i} — predicted {len(sents)} sentences:\")\n",
    "    print(\" | \".join([\" \".join(s) for s in sents]))\n",
    "\n",
    "\n",
    "evaluation.preview_predictions(best_trainer, val_ds, k=3)\n",
    "evaluation.preview_full_sentences(best_trainer, val_ds, n_examples=2)\n",
    "evaluation.preview_pred_vs_gold(best_trainer, val_ds, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4912b939",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m shown >= max_show: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m results = \u001b[43merror_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_show\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m evaluation.preview_pred_vs_gold(best_trainer, val_ds, results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36merror_examples\u001b[39m\u001b[34m(trainer, ds, max_show)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_examples\u001b[39m(trainer, ds, max_show=\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     out = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     preds = out.predictions.argmax(-\u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m     labels = out.label_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:4405\u001b[39m, in \u001b[36mTrainer.predict\u001b[39m\u001b[34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4402\u001b[39m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[32m   4403\u001b[39m \u001b[38;5;28mself\u001b[39m._memory_tracker.start()\n\u001b[32m-> \u001b[39m\u001b[32m4405\u001b[39m test_dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_test_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4406\u001b[39m start_time = time.time()\n\u001b[32m   4408\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:1236\u001b[39m, in \u001b[36mTrainer.get_test_dataloader\u001b[39m\u001b[34m(self, test_dataset)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_test_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_dataset: Dataset) -> DataLoader:\n\u001b[32m   1226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1227\u001b[39m \u001b[33;03m    Returns the test [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[32m   1228\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m \u001b[33;03m            `model.forward()` method are automatically removed. It must implement `__len__`.\u001b[39;00m\n\u001b[32m   1235\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43msampler_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_eval_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/transformers/trainer.py:1110\u001b[39m, in \u001b[36mTrainer._get_dataloader\u001b[39m\u001b[34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[39m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[32m   1106\u001b[39m         dataloader_params[\u001b[33m\"\u001b[39m\u001b[33mworker_init_fn\u001b[39m\u001b[33m\"\u001b[39m] = partial(\n\u001b[32m   1107\u001b[39m             seed_worker, num_workers=\u001b[38;5;28mself\u001b[39m.args.dataloader_num_workers, rank=\u001b[38;5;28mself\u001b[39m.args.process_index\n\u001b[32m   1108\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;66;03m# Store the prepared dataloader for subsequent evaluations if using persistent workers.\u001b[39;00m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataloader_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.dataloader_persistent_workers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1477\u001b[39m, in \u001b[36mAccelerator.prepare\u001b[39m\u001b[34m(self, device_placement, *args)\u001b[39m\n\u001b[32m   1466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1467\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(obj, torch.nn.Module)\n\u001b[32m   1468\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verify_device_map(obj)\n\u001b[32m   1469\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distributed_type != DistributedType.NO\n\u001b[32m   1470\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) != \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1471\u001b[39m     ):\n\u001b[32m   1472\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1473\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train a model that has been loaded with `device_map=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` in any distributed mode.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1474\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[33m{{\u001b[39m\u001b[33mmyscript.py}}`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1475\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistributed_type\u001b[49m == DistributedType.DEEPSPEED:\n\u001b[32m   1478\u001b[39m     model_count = \u001b[32m0\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:674\u001b[39m, in \u001b[36mAccelerator.distributed_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributed_type\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mnlp/.venv/lib/python3.12/site-packages/accelerate/state.py:1219\u001b[39m, in \u001b[36mAcceleratorState.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1216\u001b[39m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[32m   1217\u001b[39m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[32m   1218\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._known_attrs:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1220\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1221\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1222\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1223\u001b[39m         )\n\u001b[32m   1224\u001b[39m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAcceleratorState\u001b[39m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "def error_examples(trainer, ds, max_show=10):\n",
    "    out = trainer.predict(ds)\n",
    "    preds = out.predictions.argmax(-1)\n",
    "    labels = out.label_ids\n",
    "    mask = labels != -100\n",
    "    ids = ds[\"input_ids\"]\n",
    "    tok = trainer.tokenizer\n",
    "    shown = 0\n",
    "    results = []\n",
    "    for i in range(len(ds)):\n",
    "        m = mask[i]\n",
    "        if not m.any(): continue\n",
    "        y_true = labels[i][m]\n",
    "        y_pred = preds[i][m]\n",
    "        if (y_true != y_pred).any():\n",
    "            results.append(i)\n",
    "            words = tok.convert_ids_to_tokens(ds[i][\"input_ids\"])\n",
    "            visible_words = [w for w,mm in zip(words, m) if mm]\n",
    "            # mark predicted boundaries with \"▌\"\n",
    "            pieces = []\n",
    "            for w, b, t in zip(visible_words, y_pred, y_true):\n",
    "                mark = \"▌\" if b==1 else \"\"\n",
    "                pieces.append(w+mark)\n",
    "            print(\" \".join(pieces))\n",
    "            shown += 1\n",
    "            if shown >= max_show: break\n",
    "    return results\n",
    "\n",
    "results = error_examples(best_trainer, val_ds, max_show=5)\n",
    "evaluation.preview_pred_vs_gold(best_trainer, val_ds, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13570eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/deberta-v3-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379363653349465ab711f4f4eda15252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da7520c9efd4292be7bc343a18aafa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0093fd76b0456d93061a05e45e8a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answerdotai/ModernBERT-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1221aa2221b447a89c2ff1377c56945b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649e2f905eb6467988e5dac728bc58a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9f437512c748d39e4c342f6f2f67a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5df115ed944a87901e94b2347aeae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116108e42f4425ca6b465236e19ca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061d3512eeec4ad3a04994be551b8390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta OOD Results: {'precision': 0.9916666666666667, 'recall': 0.9972067039106145, 'f1': 0.9944289693593314, 'accuracy': 0.9996245893946504}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modernbert OOD Results: {'precision': 0.9763313609467456, 'recall': 0.990990990990991, 'f1': 0.9836065573770492, 'accuracy': 0.998860103626943}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert OOD Results: {'precision': 0.9895287958115183, 'recall': 0.9973614775725593, 'f1': 0.9934296977660972, 'accuracy': 0.9995317036620773}\n"
     ]
    }
   ],
   "source": [
    "# OOD\n",
    "ood_results = {}\n",
    "importlib.reload(dataset)\n",
    "\n",
    "for model_key in [\"deberta\", \"modernbert\", \"bert\"]:\n",
    "    pairs = dataset.read_token_label_file(OOD_DATA)\n",
    "    sents_tok, sents_lab = dataset.group_into_sentences(pairs)\n",
    "    ds_full = dataset.build_hf_dataset_for_token_classification(sents_tok, sents_lab, model_key=model_key)\n",
    "    ds_full.save_to_disk(HF_DATA/f\"{model_key}\"/\"ood\")\n",
    "\n",
    "# -- OOD evaluation loop:\n",
    "ood_results = {}\n",
    "for model_key in [\"deberta\", \"modernbert\", \"bert\"]:\n",
    "    model_dir = paths.chekpoints / model_key\n",
    "    trainer = evaluation.load_trainer_for_eval(model_dir, HF_DATA / model_key / \"ood\")\n",
    "    pred = trainer.predict(trainer.eval_dataset)  # use their own OOD eval set\n",
    "    logits = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    metrics = evaluation.compute_prf(logits, labels)\n",
    "    ood_results[model_key] = metrics\n",
    "    print(f\"{model_key} OOD Results:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f6a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.999438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modernbert</th>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.996997</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.999171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deberta</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>0.965462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision    recall        f1  accuracy\n",
       "bert         0.992084  0.992084  0.992084  0.999438\n",
       "modernbert   0.979351  0.996997  0.988095  0.999171\n",
       "deberta      0.187500  0.008380  0.016043  0.965462"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ood_results).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954a72c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁più ▁aff ▁\" ▁Feder ▁: ▁« ▁voi ▁a ▁una ▁bu ▁nu ▁da ▁dar ▁\" ▁e ▁me ▁la ▁fate ▁tanto ▁so ▁? ▁»▌ ▁« ▁Una ▁bu ▁nu ▁\" ▁io ▁?▌ ▁Ho ▁l ▁inferno ▁nel ▁cu ▁; ▁e ▁vi ▁dar ▁una ▁bu ▁nu ▁?▌ ▁Dit ▁voi ▁\" ▁se ▁lo ▁sap ▁\" ▁qual ▁è ▁quest ▁bu ▁nu ▁che ▁as ▁da ▁un ▁par ▁mio ▁. ▁»▌ ▁« ▁Che ▁Dio ▁v ▁ha ▁to ▁il ▁cu ▁\" ▁e ▁vu ▁far ▁suo ▁\" ▁» ▁ris ▁pa ▁il ▁cardinal ▁.▌ ▁« ▁Dio ▁! ▁Dio ▁! ▁Dio ▁!▌ ▁Se ▁lo ▁ved ▁!▌ ▁Se ▁lo ▁sent ▁!▌ ▁Dov ▁è ▁questo ▁Dio ▁? ▁»▌ ▁« ▁Voi ▁me ▁lo ▁do ▁? ▁voi ▁?▌ ▁E ▁chi ▁più ▁di ▁voi ▁l ▁ha ▁vic ▁?▌ ▁Non ▁ve ▁lo ▁sent ▁in ▁cu ▁\" ▁che ▁v ▁op ▁\" ▁che ▁v ▁a ▁\" ▁che ▁non ▁vi ▁las ▁stare ▁\" ▁e ▁n ▁st ▁tempo ▁v ▁at ▁\" ▁vi ▁fa ▁present ▁una ▁s ▁di ▁quiet ▁\" ▁di ▁cons ▁\" ▁d ▁una ▁cons ▁che ▁sar ▁pie ▁\" ▁i ▁\" ▁sub ▁che ▁voi ▁lo ▁rico ▁\" ▁lo ▁confess ▁\" ▁l ▁impl ▁? ▁»▌ ▁« ▁Oh ▁\" ▁cer ▁! ▁ho ▁qui ▁qual ▁cosa ▁che ▁m ▁op ▁\" ▁che ▁mi ▁rode ▁!▌ ▁Ma ▁Dio ▁!▌ ▁Se ▁c ▁è ▁questo ▁Dio ▁\" ▁se ▁è ▁quell ▁che ▁di ▁\" ▁cosa ▁vol ▁che ▁f ▁di ▁me ▁? ▁»▌ ▁Quest ▁parole ▁fur ▁de ▁con ▁un ▁accent ▁disp ▁; ▁ma ▁Feder ▁\" ▁con ▁un ▁to ▁sole ▁\" ▁come ▁di ▁placid ▁is ▁\" ▁ris ▁: ▁« ▁cosa ▁pu ▁far ▁Dio ▁di ▁voi ▁? ▁cosa ▁vu ▁far ▁?▌ ▁Un ▁seg ▁della ▁sua ▁pot ▁e ▁della ▁sua ▁bon ▁: ▁vu ▁ca ▁da ▁voi ▁una ▁glo ▁che ▁ness ▁al ▁gli ▁pot ▁dare ▁.▌ ▁Che ▁il ▁mondo ▁grid ▁da ▁tanto ▁tempo ▁contro ▁di ▁voi ▁\" ▁che ▁mill ▁e ▁mill ▁voc ▁detest ▁le ▁vos ▁op ▁. ▁» ▁( ▁l ▁in ▁si ▁sc ▁\" ▁e ▁rim ▁stu ▁un ▁momento ▁nel ▁sent ▁quel ▁lingua ▁cos ▁in ▁\"\n",
      "▁\" ▁e ▁in ▁più ▁minut ▁dai ▁pic ▁compar ▁delle ▁vet ▁.▌ ▁Un ▁qual ▁demon ▁\" ▁o ▁. ▁un ▁qual ▁angel ▁che ▁la ▁prote ▁.▌ ▁.▌ ▁Compassion ▁al ▁Nib ▁! ▁.▌ ▁Do ▁\" ▁do ▁di ▁bu ▁or ▁\" ▁fu ▁di ▁qui ▁cost ▁; ▁al ▁suo ▁des ▁\" ▁e ▁non ▁se ▁ne ▁par ▁più ▁\" ▁e ▁\" ▁– ▁prose ▁tra ▁sé ▁\" ▁con ▁quell ▁an ▁con ▁cui ▁si ▁com ▁a ▁un ▁rag ▁in ▁\" ▁sap ▁che ▁non ▁u ▁\" ▁– ▁e ▁non ▁ci ▁si ▁pens ▁più ▁.▌ ▁Que ▁animal ▁di ▁don ▁Rodrigo ▁non ▁mi ▁v ▁a ▁romper ▁la ▁test ▁con ▁ring ▁; ▁che ▁. ▁non ▁v ▁più ▁sent ▁par ▁di ▁cost ▁.▌ ▁L ▁ho ▁servi ▁per ▁. ▁per ▁ho ▁prom ▁: ▁e ▁ho ▁prom ▁per ▁. ▁è ▁il ▁mio ▁des ▁.▌ ▁Ma ▁v ▁che ▁me ▁lo ▁p ▁be ▁questo ▁servi ▁\" ▁co ▁.▌ ▁Ved ▁un ▁poco ▁. ▁–▌ ▁E ▁vol ▁almanac ▁cosa ▁a ▁pot ▁rich ▁di ▁scab ▁\" ▁per ▁comp ▁\" ▁e ▁quasi ▁per ▁pen ▁; ▁ma ▁gli ▁si ▁at ▁di ▁nu ▁all ▁men ▁quell ▁parole ▁: ▁compassion ▁al ▁Nib ▁!▌ ▁– ▁Come ▁pu ▁a ▁fat ▁cost ▁? ▁– ▁continua ▁\" ▁stra ▁da ▁quel ▁pens ▁.▌ ▁– ▁V ▁ved ▁.▌ ▁Eh ▁! ▁no ▁.▌ ▁S ▁\" ▁v ▁ved ▁. ▁–▌ ▁E ▁d ▁una ▁stanza ▁in ▁un ▁al ▁\" ▁t ▁una ▁scale ▁\" ▁e ▁su ▁a ▁ta ▁\" ▁and ▁all ▁camera ▁della ▁ve ▁\" ▁e ▁pic ▁all ▁us ▁con ▁un ▁calci ▁.▌ ▁« ▁Chi ▁è ▁? ▁»▌ ▁« ▁Apr ▁. ▁»▌ ▁A ▁quell ▁voce ▁\" ▁la ▁ve ▁f ▁tre ▁salt ▁; ▁e ▁sub ▁si ▁sent ▁sc ▁il ▁pal ▁negli ▁an ▁\" ▁e ▁l ▁us ▁si ▁spa ▁.▌ ▁L ▁in ▁\" ▁dalla ▁so ▁\" ▁died ▁un ▁ ▁in ▁g ▁; ▁e ▁\" ▁al ▁ ▁d ▁una ▁luc ▁che ▁a ▁sur ▁un ▁ta\n",
      "e ▁\" ▁et ▁m ▁pen ▁pec ▁et ▁an ▁corporal ▁sino ▁all ▁gal ▁\" ▁all ▁ ▁di ▁S ▁\" ▁second ▁la ▁qual ▁de ▁casi ▁et ▁delle ▁person ▁.▌ ▁Al ▁ris ▁b ▁era ▁gi ▁stato ▁f ▁il ▁prezzo ▁prima ▁della ▁so ▁; ▁come ▁prob ▁la ▁tariff ▁o ▁\" ▁per ▁us ▁quell ▁den ▁celeb ▁negli ▁an ▁modern ▁\" ▁il ▁maximum ▁del ▁gran ▁e ▁dell ▁alt ▁gran ▁più ▁or ▁sar ▁stato ▁f ▁con ▁alt ▁grid ▁\" ▁che ▁non ▁c ▁è ▁av ▁di ▁ve ▁.▌ ▁Man ▁cos ▁il ▁pane ▁e ▁la ▁far ▁a ▁bu ▁merc ▁in ▁Milano ▁\" ▁ne ▁veni ▁di ▁cons ▁che ▁dalla ▁camp ▁acc ▁gente ▁a ▁procession ▁a ▁comprar ▁.▌ ▁Don ▁Gonzalo ▁\" ▁per ▁rip ▁a ▁questo ▁\" ▁come ▁dice ▁lui ▁\" ▁inconvenient ▁\" ▁pro ▁\" ▁con ▁un ▁al ▁grid ▁del ▁15 ▁di ▁dic ▁\" ▁di ▁port ▁fu ▁della ▁cit ▁pane ▁\" ▁per ▁più ▁del ▁valor ▁di ▁vent ▁sold ▁; ▁pen ▁la ▁per ▁del ▁pane ▁me ▁\" ▁e ▁vent ▁sc ▁\" ▁et ▁in ▁caso ▁di ▁in ▁\" ▁di ▁due ▁tra ▁di ▁cord ▁in ▁public ▁\" ▁et ▁m ▁pen ▁an ▁\" ▁second ▁il ▁so ▁\" ▁all ▁ ▁di ▁S ▁Il ▁22 ▁dello ▁st ▁me ▁( ▁e ▁non ▁si ▁ve ▁per ▁cos ▁t ▁) ▁\" ▁pub ▁un ▁or ▁so ▁per ▁le ▁far ▁e ▁per ▁i ▁gran ▁.▌ ▁La ▁molt ▁a ▁vol ▁far ▁nas ▁l ▁ab ▁col ▁sac ▁e ▁con ▁l ▁in ▁; ▁il ▁govern ▁vol ▁man ▁con ▁la ▁gal ▁e ▁con ▁la ▁cord ▁.▌ ▁I ▁me ▁era ▁convenient ▁tra ▁loro ▁; ▁ma ▁cosa ▁a ▁a ▁fare ▁col ▁fine ▁\" ▁il ▁let ▁lo ▁ve ▁: ▁come ▁va ▁in ▁fat ▁ad ▁o ▁\" ▁lo ▁ved ▁a ▁moment ▁.▌ ▁ ▁poi ▁facile ▁anche ▁ve ▁\" ▁e ▁non ▁in ▁l ▁oss ▁come ▁tra ▁que ▁stran ▁pro\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0 — 6 predicted sentences:\n",
      " • a cui dove tir il collo \" per il ban di dome \" e port ; per non bis mai and con le mani v da que sign .\n",
      " • Rac tutto l acc ; e ved che vi di \" su due pie \" di quell co che a no non ver in test \" a pens un an . »\n",
      " • Renzo ab molto vol questo pare ; Lucia l app ; e Agnes \" superb d a da \" lev \" a una a una \" le po best dalla st \" ri le loro gamb \" come se faces un m di fi \" le av e le st con uno spa \" e le cons in man a Renzo ; il qual \" date e rice parole di s \" us dalla parte dell or \" per non ved da rag \" che gli corre diet \" grid : lo spo ! lo spo ! Co \"\n",
      " • at i camp o \" come di col \" i lu \" se n and per vi \" fre \" ripen all sua dis \" e rum il disc da fare al do Azz . Las poi\n",
      " • pens al let \" come doves stare in via quell po best \" cos leg e ten per le za \" a capo all in gi \" nella man d un u il qual \" a da t passion \" accom col gest i pens che gli pass a tumult per la men . Ora st\n",
      " • il bra per colle \" or l a per dis \" or lo di in a \"\n",
      "\n",
      "Example 1 — 18 predicted sentences:\n",
      " • più aff \" Feder : « voi a una bu nu da dar \" e me la fate tanto so ? »\n",
      " • « Una bu nu \" io ?\n",
      " • Ho l inferno nel cu ; e vi dar una bu nu ?\n",
      " • Dit voi \" se lo sap \" qual è quest bu nu che as da un par mio . »\n",
      " • « Che Dio v ha to il cu \" e vu far suo \" » ris pa il cardinal .\n",
      " • « Dio ! Dio ! Dio !\n",
      " • Se lo ved !\n",
      " • Se lo sent !\n",
      " • Dov è questo Dio ? »\n",
      " • « Voi me lo do ? voi ?\n",
      " • E chi più di voi l ha vic ?\n",
      " • Non ve lo sent in cu \" che v op \" che v a \" che non vi las stare \" e n st tempo v at \" vi fa present una s di quiet \" di cons \" d una cons che sar pie \" i \" sub che voi lo rico \" lo confess \" l impl ? »\n",
      " • « Oh \" cer ! ho qui qual cosa che m op \" che mi rode !\n",
      " • Ma Dio !\n",
      " • Se c è questo Dio \" se è quell che di \" cosa vol che f di me ? »\n",
      " • Quest parole fur de con un accent disp ; ma Feder \" con un to sole \" come di placid is \" ris : « cosa pu far Dio di voi ? cosa vu far ?\n",
      " • Un seg della sua pot e della sua bon : vu ca da voi una glo che ness al gli pot dare .\n",
      " • Che il mondo grid da tanto tempo contro di voi \" che mill e mill voc detest le vos op . » ( l in si sc \" e rim stu un momento nel sent quel lingua cos in \"\n",
      "\n",
      "Example 2 — 18 predicted sentences:\n",
      " • i servi delle due part si sl all dif de loro pad .\n",
      " • Il combat era dis \" e per il numero \" e anche per Lod mi pi a scans i col \" e a disarm il ne \" che ad ; ma questo vol la mort di lui \" a ogni cost . Lod\n",
      " • a gi rice al bra sin una pug d un bravo \" e una s leg in una guan \" e il ne principal gli pi add per fin ; quando Cristo \" ved il suo pad ne est per \" and col pug add al sign . Quest\n",
      " • \" riv t la sua i contro di lui \" lo pass con la spa . A\n",
      " • quell vista \" Lod \" come fu di sé \" c la sua nel vent del fer \" il qual cad mori \" quasi a un punto col pov Cristo . I\n",
      " • bra del gent \" visto ch era fin \" si died all fug \" mal : quell di Lod \" tart e sf anche loro \" non essen più a chi dare \" e non vol trova imp nella gente \" che gi acc \" scan da al parte : e Lod si t solo \" con que due fun comp a pie \" in mezzo a una foll . «\n",
      " • Com è and ? –\n",
      " • uno . – Son\n",
      " • due . – Gli\n",
      " • ha fat un nel vent . – Chi è\n",
      " • stato a ? – Quel pre\n",
      " • . – Oh santa\n",
      " • Maria \" che s ! – Chi cerca\n",
      " • trova . – Una le\n",
      " • p tutte . – Ha fin\n",
      " • anche lui . – Che col\n",
      " • ! – Vu essere\n",
      " • una fac s . – E quell\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Example 0 — 6 predicted sentences\n",
      "\n",
      " • a cui dovevo tirare il collo \",\" per il banchetto di domenica \",\" e portateglieli ; perché non bisogna mai andar con le mani vòte da que' signori . Rac\n",
      " • contategli tutto l' accaduto ; e vedrete che vi dirà \",\" su due piedi \",\" di quelle cose che a noi non verrebbero in testa \",\" a pensarci un anno . » Renzo\n",
      " • abbracciò molto volentieri questo parere ; Lucia l' approvò ; e Agnese \",\" superba d' averlo dato \",\" levò \",\" a una a una \",\" le povere bestie dalla stìa \",\" riunì le loro otto gambe \",\" come se facesse un mazzetto di fiori \",\" le avvolse e le strinse con uno spago \",\" e le consegnò in mano a Renzo ; il quale \",\" date e ricevute parole di speranza \",\" uscì dalla parte dell' orto \",\" per non esser veduto da' ragazzi \",\" che gli correrebber dietro \",\" gridando : lo sposo ! lo sposo ! Co\n",
      " • sì \",\" attraversando i campi o \",\" come dicon colà \",\" i luoghi \",\" se n' andò per viottole \",\" fremendo \",\" ripensando alla sua disgrazia \",\" e ruminando il discorso da fare al dottor Azzecca-garbugli . Las\n",
      " • cio poi pensare al lettore \",\" come dovessero stare in viaggio quelle povere bestie \",\" così legate e tenute per le zampe \",\" a capo all' in giù \",\" nella mano d' un uomo il quale \",\" agitato da tante passioni \",\" accompagnava col gesto i pensieri che gli passavan a tumulto per la mente . Ora\n",
      " • stendeva il braccio per collera \",\" ora l' alzava per disperazione \",\" ora lo dibatteva in aria \",\"\n",
      "\n",
      "### Example 1 — 18 predicted sentences\n",
      "\n",
      " • più affettuosamente \",\" Federigo : « voi avete una buona nuova da darmi \",\" e me la fate tanto sospirare ? » «\n",
      " • Una buona nuova \",\" io ? Ho\n",
      " • l' inferno nel cuore ; e vi darò una buona nuova ? Dit\n",
      " • emi voi \",\" se lo sapete \",\" qual è questa buona nuova che aspettate da un par mio . » «\n",
      " • Che Dio v' ha toccato il cuore \",\" e vuol farvi suo \",\" » rispose pacatamente il cardinale . «\n",
      " • Dio ! Dio ! Dio ! Se\n",
      " • lo vedessi ! Se\n",
      " • lo sentissi ! Dov\n",
      " • ' è questo Dio ? » «\n",
      " • Voi me lo domandate ? voi ? E\n",
      " • chi più di voi l' ha vicino ? Non\n",
      " • ve lo sentite in cuore \",\" che v' opprime \",\" che v' agita \",\" che non vi lascia stare \",\" e nello stesso tempo v' attira \",\" vi fa presentire una speranza di quiete \",\" di consolazione \",\" d' una consolazione che sarà piena \",\" immensa \",\" subito che voi lo riconosciate \",\" lo confessiate \",\" l' imploriate ? » «\n",
      " • Oh \",\" certo ! ho qui qualche cosa che m' opprime \",\" che mi rode ! Ma\n",
      " • Dio ! Se\n",
      " • c' è questo Dio \",\" se è quello che dicono \",\" cosa volete che faccia di me ? » Quest\n",
      " • e parole furon dette con un accento disperato ; ma Federigo \",\" con un tono solenne \",\" come di placida ispirazione \",\" rispose : « cosa può far Dio di voi ? cosa vuol farne ? Un\n",
      " • segno della sua potenza e della sua bontà : vuol cavar da voi una gloria che nessun altro gli potrebbe dare . Che\n",
      " • il mondo gridi da tanto tempo contro di voi \",\" che mille e mille voci detestino le vostre opere ... » ( l' innominato si scosse \",\" e rimase stupefatto un momento nel sentir quel linguaggio così insolito \",\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Window 1\n",
      "P: più affettuosamente \",\" Federigo : « voi avete una buona nuova da darmi \",\" e me la fate tanto sospirare ? » « | Una buona nuova \",\" io ? Ho | l' inferno nel cuore ; e vi darò una buona nuova ? Dit | emi voi \",\" se lo sapete \",\" qual è questa buona nuova che aspettate da un par mio . » « | Che Dio v' ha toccato il cuore \",\" e vuol farvi suo \",\" » rispose pacatamente il cardinale . « | Dio ! Dio ! Dio ! Se | lo vedessi ! Se | lo sentissi ! Dov | ' è questo Dio ? » « | Voi me lo domandate ? voi ? E | chi più di voi l' ha vicino ? Non | ve lo sentite in cuore \",\" che v' opprime \",\" che v' agita \",\" che non vi lascia stare \",\" e nello stesso tempo v' attira \",\" vi fa presentire una speranza di quiete \",\" di consolazione \",\" d' una consolazione che sarà piena \",\" immensa \",\" subito che voi lo riconosciate \",\" lo confessiate \",\" l' imploriate ? » « | Oh \",\" certo ! ho qui qualche cosa che m' opprime \",\" che mi rode ! Ma | Dio ! Se | c' è questo Dio \",\" se è quello che dicono \",\" cosa volete che faccia di me ? » Quest | e parole furon dette con un accento disperato ; ma Federigo \",\" con un tono solenne \",\" come di placida ispirazione \",\" rispose : « cosa può far Dio di voi ? cosa vuol farne ? Un | segno della sua potenza e della sua bontà : vuol cavar da voi una gloria che nessun altro gli potrebbe dare . Che | il mondo gridi da tanto tempo contro di voi \",\" che mille e mille voci detestino le vostre opere ... » ( l' innominato si scosse \",\" e rimase stupefatto un momento nel sentir quel linguaggio così insolito \",\n",
      "G: più affettuosamente \",\" Federigo : « voi avete una buona nuova da darmi \",\" e me la fate tanto sospirare ? » « | Una buona nuova \",\" io ? Ho | l' inferno nel cuore ; e vi darò una buona nuova ? Dit | emi voi \",\" se lo sapete \",\" qual è questa buona nuova che aspettate da un par mio . » « | Che Dio v' ha toccato il cuore \",\" e vuol farvi suo \",\" » rispose pacatamente il cardinale . « | Dio ! Dio | ! Dio | ! Se | lo vedessi ! Se | lo sentissi ! Dov | ' è questo Dio ? » « | Voi me lo domandate ? voi ? E | chi più di voi l' ha vicino ? Non | ve lo sentite in cuore \",\" che v' opprime \",\" che v' agita \",\" che non vi lascia stare \",\" e nello stesso tempo v' attira \",\" vi fa presentire una speranza di quiete \",\" di consolazione \",\" d' una consolazione che sarà piena \",\" immensa \",\" subito che voi lo riconosciate \",\" lo confessiate \",\" l' imploriate ? » « | Oh \",\" certo ! ho qui qualche cosa che m' opprime \",\" che mi rode ! Ma | Dio ! Se | c' è questo Dio \",\" se è quello che dicono \",\" cosa volete che faccia di me ? » Quest | e parole furon dette con un accento disperato ; ma Federigo \",\" con un tono solenne \",\" come di placida ispirazione \",\" rispose : « cosa può far Dio di voi ? cosa vuol farne ? Un | segno della sua potenza e della sua bontà : vuol cavar da voi una gloria che nessun altro gli potrebbe dare . Che | il mondo gridi da tanto tempo contro di voi \",\" che mille e mille voci detestino le vostre opere ... » ( l' innominato si scosse \",\" e rimase stupefatto un momento nel sentir quel linguaggio così insolito \",\n",
      "\n",
      "### Window 35\n",
      "P: \",\" e intagliata più minutamente dai piccoli compartimenti delle vetriate . Un | qualche demonio \",\" o ... un qualche angelo che la protegge . . | Compassion | e al Nibbio ! .. | . Domattina \",\" domattina di buon' ora \",\" fuor di qui costei ; al suo destino \",\" e non se ne parli più \",\" e \",\" – proseguiva tra sé \",\" con quell' animo con cui si comanda a un ragazzo indocile \",\" sapendo che non ubbidirà \",\" – e non ci si pensi più . Que | ll' animale di don Rodrigo non mi venga a romper la testa con ringraziamenti ; che ... non voglio più sentir parlar di costei . L | ' ho servito perché ... perché ho promesso : e ho promesso perché ... è il mio destino . Ma | voglio che me lo paghi bene questo servizio \",\" colui . Ved | iamo un poco ... – E | voleva almanaccare cosa avrebbe potuto richiedergli di scabroso \",\" per compenso \",\" e quasi per pena ; ma gli si attraversaron di nuovo alla mente quelle parole : compassione al Nibbio ! – | Come può aver fatto costei ? – continuava \",\" strascinato da quel pensiero . – | Voglio vederla .. | . Eh ! no .. | . Sì \",\" voglio vederla . – E | d' una stanza in un' altra \",\" trovò una scaletta \",\" e su a tastone \",\" andò alla camera della vecchia \",\" e picchiò all' uscio con un calcio . « | Chi è ? » « | Apri . » A | quella voce \",\" la vecchia fece tre salti ; e subito si sentì scorrere il paletto negli anelli \",\" e l' uscio si spalancò . L | ' innominato \",\" dalla soglia \",\" diede un' occhiata in giro ; e \",\" al lume d' una lucerna che ardeva sur un ta\n",
      "G: \",\" e intagliata più minutamente dai piccoli compartimenti delle vetriate . Un | qualche demonio \",\" o ... un qualche angelo che la protegge . . Compassion | e al Nibbio ! .. | . Domattina \",\" domattina di buon' ora \",\" fuor di qui costei ; al suo destino \",\" e non se ne parli più \",\" e \",\" – proseguiva tra sé \",\" con quell' animo con cui si comanda a un ragazzo indocile \",\" sapendo che non ubbidirà \",\" – e non ci si pensi più . Que | ll' animale di don Rodrigo non mi venga a romper la testa con ringraziamenti ; che ... non voglio più sentir parlar di costei . L | ' ho servito perché ... perché ho promesso : e ho promesso perché ... è il mio destino . Ma | voglio che me lo paghi bene questo servizio \",\" colui . Ved | iamo un poco ... – E | voleva almanaccare cosa avrebbe potuto richiedergli di scabroso \",\" per compenso \",\" e quasi per pena ; ma gli si attraversaron di nuovo alla mente quelle parole : compassione al Nibbio ! – | Come può aver fatto costei ? – continuava \",\" strascinato da quel pensiero . – | Voglio vederla .. | . Eh ! no .. | . Sì \",\" voglio vederla . – E | d' una stanza in un' altra \",\" trovò una scaletta \",\" e su a tastone \",\" andò alla camera della vecchia \",\" e picchiò all' uscio con un calcio . « | Chi è ? » « | Apri . » A | quella voce \",\" la vecchia fece tre salti ; e subito si sentì scorrere il paletto negli anelli \",\" e l' uscio si spalancò . L | ' innominato \",\" dalla soglia \",\" diede un' occhiata in giro ; e \",\" al lume d' una lucerna che ardeva sur un ta\n",
      "\n",
      "### Window 48\n",
      "P: e \",\" et maggior pena pecuniaria et ancora corporale sino alla galera \",\" all' arbitrio di S.E. \",\" secondo la qualità de' casi et delle persone . Al | riso brillato era già stato fissato il prezzo prima della sommossa ; come probabilmente la tariffa o \",\" per usare quella denominazione celeberrima negli annali moderni \",\" il maximum del grano e dell' altre granaglie più ordinarie sarà stato fissato con altre gride \",\" che non c' è avvenuto di vedere . Man | tenuto così il pane e la farina a buon mercato in Milano \",\" ne veniva di conseguenza che dalla campagna accorresse gente a processione a comprarne . Don | Gonzalo \",\" per riparare a questo \",\" come dice lui \",\" inconveniente \",\" proibì \",\" con un' altra grida del 15 di dicembre \",\" di portar fuori della città pane \",\" per più del valore di venti soldi ; pena la perdita del pane medesimo \",\" e venticinque scudi \",\" et in caso di inhabilità \",\" di due tratti di corda in publico \",\" et maggior pena ancora \",\" secondo il solito \",\" all' arbitrio di S.E. Il 22 dello stesso mese ( e non si vede perché così tardi ) \",\" pubblicò un ordine somigliante per le farine e per i grani . La | moltitudine aveva voluto far nascere l' abbondanza col saccheggio e con l' incendio ; il governo voleva mantenerla con la galera e con la corda . I | mezzi erano convenienti tra loro ; ma cosa avessero a fare col fine \",\" il lettore lo vede : come valessero in fatto ad ottenerlo \",\" lo vedrà a momenti . | È poi facile anche vedere \",\" e non inutile l' osservare come tra quegli strani provvedimenti\n",
      "G: e \",\" et maggior pena pecuniaria et ancora corporale sino alla galera \",\" all' arbitrio di S.E. \",\" secondo la qualità de' casi et delle persone . Al | riso brillato era già stato fissato il prezzo prima della sommossa ; come probabilmente la tariffa o \",\" per usare quella denominazione celeberrima negli annali moderni \",\" il maximum del grano e dell' altre granaglie più ordinarie sarà stato fissato con altre gride \",\" che non c' è avvenuto di vedere . Man | tenuto così il pane e la farina a buon mercato in Milano \",\" ne veniva di conseguenza che dalla campagna accorresse gente a processione a comprarne . Don | Gonzalo \",\" per riparare a questo \",\" come dice lui \",\" inconveniente \",\" proibì \",\" con un' altra grida del 15 di dicembre \",\" di portar fuori della città pane \",\" per più del valore di venti soldi ; pena la perdita del pane medesimo \",\" e venticinque scudi \",\" et in caso di inhabilità \",\" di due tratti di corda in publico \",\" et maggior pena ancora \",\" secondo il solito \",\" all' arbitrio di S. | E. Il 22 dello stesso mese ( e non si vede perché così tardi ) \",\" pubblicò un ordine somigliante per le farine e per i grani . La | moltitudine aveva voluto far nascere l' abbondanza col saccheggio e con l' incendio ; il governo voleva mantenerla con la galera e con la corda . I | mezzi erano convenienti tra loro ; ma cosa avessero a fare col fine \",\" il lettore lo vede : come valessero in fatto ad ottenerlo \",\" lo vedrà a momenti . | È poi facile anche vedere \",\" e non inutile l' osservare come tra quegli strani provvedimenti\n"
     ]
    }
   ],
   "source": [
    "best_key = max(ood_results, key=lambda k: ood_results[k][\"f1\"])\n",
    "model_dir = paths.chekpoints/best_key\n",
    "\n",
    "ood_best_trainer = evaluation.load_trainer_for_eval(model_dir, HF_DATA / best_key / \"ood\")\n",
    "ood_ds = load_from_disk(HF_DATA/best_key/\"val\")\n",
    "results = error_examples(ood_best_trainer, ood_ds, max_show=5)\n",
    "\n",
    "evaluation.preview_predictions(ood_best_trainer, ood_ds, k=3)\n",
    "evaluation.preview_full_sentences(ood_best_trainer, ood_ds, n_examples=2)\n",
    "evaluation.preview_pred_vs_gold(ood_best_trainer, ood_ds, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed9c0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deberta] saved VAL predictions: {'n_samples': 60, 'n_rows': 17305, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/deberta_val_tokens.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deberta] saved OOD predictions: {'n_samples': 37, 'n_rows': 10655, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/deberta_ood_tokens.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modernbert] saved VAL predictions: {'n_samples': 14, 'n_rows': 16499, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/modernbert_val_tokens.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modernbert] saved OOD predictions: {'n_samples': 9, 'n_rows': 9650, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/modernbert_ood_tokens.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bert] saved VAL predictions: {'n_samples': 53, 'n_rows': 17274, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/bert_val_tokens.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mnlp/notebooks/../src/evaluation.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bert] saved OOD predictions: {'n_samples': 33, 'n_rows': 10677, 'word_only': True, 'path': '/home/user/mnlp/notebooks/../results/bert_ood_tokens.csv'}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import evaluation\n",
    "from datasets import load_from_disk\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "model_keys = [\"deberta\", \"modernbert\", \"bert\"]\n",
    "\n",
    "for model_key in model_keys:\n",
    "    model_dir = paths.chekpoints / model_key\n",
    "\n",
    "    # --- Validation ---\n",
    "    val_ds_path = HF_DATA / model_key / \"val\"\n",
    "    val_ds = load_from_disk(val_ds_path)\n",
    "    val_tr = evaluation.load_trainer_for_eval(model_dir, val_ds_path)\n",
    "    val_out = paths.results / f\"{model_key}_val_tokens.csv\"   # or .parquet / .jsonl\n",
    "    _, val_summary = evaluation.save_token_predictions(val_tr, val_ds, val_out, word_only=True)\n",
    "    print(f\"[{model_key}] saved VAL predictions:\", val_summary)\n",
    "\n",
    "    # --- OOD ---\n",
    "    ood_ds_path = HF_DATA / model_key / \"ood\"\n",
    "    ood_ds = load_from_disk(ood_ds_path)\n",
    "    ood_tr = evaluation.load_trainer_for_eval(model_dir, ood_ds_path)\n",
    "    ood_out = paths.results / f\"{model_key}_ood_tokens.csv\"   # or .parquet / .jsonl\n",
    "    _, ood_summary = evaluation.save_token_predictions(ood_tr, ood_ds, ood_out, word_only=True)\n",
    "    print(f\"[{model_key}] saved OOD predictions:\", ood_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
